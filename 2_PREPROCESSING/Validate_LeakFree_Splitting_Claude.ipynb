{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% [markdown]<br>\n",
    "# Comprehensive Validation of Leak-Free TimeSeriesSplitter<br>\n",
    "<br>\n",
    "This notebook validates that our new TimeSeriesSplitter creates proper train/val/test splits <br>\n",
    "with appropriate data flow allowances using the visualization tools from analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "from rastermap import Rastermap\n",
    "\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from utils.data_splitter import TimeSeriesSplitter\n",
    "from utils.load_meso_session import MesoscopeSession\n",
    "from Activity_Data_Loader import Dataset_Activity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%<br>\n",
    "Load preprocessed session data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOADING SESSION DATA ===\n"
     ]
    }
   ],
   "source": [
    "print(\"=== LOADING SESSION DATA ===\")\n",
    "session_path = \"../DATA/session_61f260e7-b5d3-4865-a577-bcfc53fda8a8.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load via MesoscopeSession for trial data access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed session loaded from ../DATA/session_61f260e7-b5d3-4865-a577-bcfc53fda8a8.h5\n",
      "Shape: (19081, 6903) (time_points x neurons)\n",
      "EID: 61f260e7-b5d3-4865-a577-bcfc53fda8a8, Subject: SP066\n"
     ]
    }
   ],
   "source": [
    "preprocessed_session = MesoscopeSession.from_preprocessed(session_path)\n",
    "activity, timestamps = preprocessed_session.get_preprocessed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load covariate matrix directly from HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(session_path, 'r') as f:\n",
    "    covariate_matrix = f['covariate_matrix'][:]\n",
    "    feature_names = [name.decode('utf-8') for name in f['covariate_metadata']['feature_names'][:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity shape: (19081, 6903)\n",
      "Covariate shape: (19081, 11)\n",
      "Covariate features: ['wheel_velocity', 'stimulus_catch_trial', 'stimulus_left_100pct', 'stimulus_left_25pct', 'stimulus_left_12.5pct', 'stimulus_left_6.25pct', 'stimulus_right_100pct', 'stimulus_right_25pct', 'stimulus_right_12.5pct', 'stimulus_right_6.25pct', 'trial_phase']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Activity shape: {activity.shape}\")\n",
    "print(f\"Covariate shape: {covariate_matrix.shape}\")  \n",
    "print(f\"Covariate features: {feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CREATING STIMULUS-BASED SPLITS ===\n",
      "Stimulus type 0: 62 blocks -> 43 train, 12 val, 7 test\n",
      "Stimulus type 1: 31 blocks -> 21 train, 6 val, 4 test\n",
      "Stimulus type 2: 30 blocks -> 21 train, 6 val, 3 test\n",
      "Stimulus type 3: 19 blocks -> 13 train, 3 val, 3 test\n",
      "Stimulus type 4: 35 blocks -> 24 train, 7 val, 4 test\n",
      "Stimulus type 5: 34 blocks -> 23 train, 6 val, 5 test\n",
      "Stimulus type 6: 26 blocks -> 18 train, 5 val, 3 test\n",
      "Stimulus type 7: 40 blocks -> 28 train, 8 val, 4 test\n",
      "Stimulus type 8: 36 blocks -> 25 train, 7 val, 4 test\n",
      "Generating leak-aware sample indices...\n",
      "Found 13256 valid training samples.\n",
      "Found 2944 valid validation samples.\n",
      "Found 2818 valid test samples.\n",
      "\n",
      "Split Summary:\n",
      "  train_samples: 13256\n",
      "  val_samples: 2944\n",
      "  test_samples: 2818\n",
      "  total_samples: 19018\n",
      "  train_pct: 69.7%\n",
      "  val_pct: 15.5%\n",
      "  test_pct: 14.8%\n"
     ]
    }
   ],
   "source": [
    "# Create stimulus-based splits and TimeSeriesSplitter\n",
    "print(\"\\n=== CREATING STIMULUS-BASED SPLITS ===\")\n",
    "\n",
    "split_map = TimeSeriesSplitter.create_stimulus_based_splits(\n",
    "    covariate_matrix=covariate_matrix,\n",
    "    train_pct=0.7,\n",
    "    val_pct=0.2,\n",
    "    held_out_stimulus_types=[]  # No held-out types\n",
    ")\n",
    "\n",
    "# Test with realistic DLinear parameters\n",
    "seq_len, pred_len, label_len = 48, 16, 4\n",
    "\n",
    "splitter = TimeSeriesSplitter(\n",
    "    split_map=split_map,\n",
    "    seq_len=seq_len,\n",
    "    pred_len=pred_len,\n",
    "    label_len=label_len\n",
    ")\n",
    "\n",
    "summary = splitter.get_split_summary()\n",
    "print(f\"\\nSplit Summary:\")\n",
    "for key, value in summary.items():\n",
    "    if 'pct' in key:\n",
    "        print(f\"  {key}: {value:.1f}%\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13256"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices = splitter.get_indices('train')\n",
    "len(train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 13256 and the array at index 1 has size 2944",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stacked_indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/mpci-transformer/ibl_env/lib/python3.10/site-packages/numpy/_core/shape_base.py:292\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    291\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m (arrs,)\n\u001b[0;32m--> 292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 13256 and the array at index 1 has size 2944"
     ]
    }
   ],
   "source": [
    "stacked_indices = np.vstack((train_indices, val_indices, test_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VISUALIZING SPLIT MAP ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Rastermap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m xmin_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m xmax_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mRastermap\u001b[49m(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;66;03m# number of clusters to compute\u001b[39;00m\n\u001b[1;32m      8\u001b[0m                   n_PCs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, \u001b[38;5;66;03m# number of PCs\u001b[39;00m\n\u001b[1;32m      9\u001b[0m                   locality\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;66;03m# locality in sorting is low here to get more global sorting (this is a value from 0-1)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m                   time_lag_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;66;03m# use future timepoints to compute correlation\u001b[39;00m\n\u001b[1;32m     11\u001b[0m                   grid_upsample\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;66;03m# default value, 10 is good for large recordings\u001b[39;00m\n\u001b[1;32m     12\u001b[0m                   )\u001b[38;5;241m.\u001b[39mfit(activity\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m     14\u001b[0m y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39membedding \n\u001b[1;32m     15\u001b[0m isort \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39misort\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Rastermap' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== VISUALIZING SPLIT MAP ===\")\n",
    "\n",
    "# Define time window for visualization (in sample indices)\n",
    "xmin_idx = 0\n",
    "xmax_idx = 1000\n",
    "\n",
    "model = Rastermap(n_clusters=100, # number of clusters to compute\n",
    "                  n_PCs=200, # number of PCs\n",
    "                  locality=0.5, # locality in sorting is low here to get more global sorting (this is a value from 0-1)\n",
    "                  time_lag_window=5, # use future timepoints to compute correlation\n",
    "                  grid_upsample=10, # default value, 10 is good for large recordings\n",
    "                  ).fit(activity.T)\n",
    "\n",
    "y = model.embedding \n",
    "isort = model.isort\n",
    "\n",
    "X_embedding = model.X_embedding\n",
    "fig = plt.figure(figsize=(12,5), dpi=200)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(X_embedding, vmin=0, vmax=0.8, cmap=\"gray_r\", aspect=\"auto\")\n",
    "\n",
    "def create_split_overlay(split_maps, xmin_idx, xmax_idx):\n",
    "    \"\"\"Create a colored overlay for stimulus types\"\"\"\n",
    "    # Get stimulus data for the time window\n",
    "    stim_window = stimulus_onehot[xmin_idx:xmax_idx]\n",
    "    \n",
    "    # Define colors for each stimulus type\n",
    "    # 0: Catch trials (purple)\n",
    "    # 1-4: Left stimuli (different shades of red)\n",
    "    # 5-8: Right stimuli (different shades of blue)\n",
    "    colors = [\n",
    "        [0.5, 0, 0.5, 0.5],  # 0: Purple (catch trials - no stimulus)\n",
    "        [1, 0, 0, 0.5],      # 1: Left 100% - dark red\n",
    "        [1, 0.3, 0.3, 0.5],  # 2: Left 25% - medium red\n",
    "    ]\n",
    "    \n",
    "    # Convert one-hot to stimulus type indices\n",
    "    stim_types = np.argmax(stim_window, axis=1)\n",
    "    \n",
    "    # Create RGB overlay\n",
    "    nn = X_embedding.shape[0]  # Number of neurons\n",
    "    overlay = np.zeros((nn, len(stim_types), 4))  # RGBA\n",
    "    \n",
    "    for t, stim_type in enumerate(stim_types):\n",
    "        if np.any(stim_window[t]):  # Only color if there's a stimulus\n",
    "            overlay[:, t] = colors[stim_type]\n",
    "    \n",
    "    return overlay\n",
    "\n",
    "# Create figure for stimulus-colored rastermap with wheel velocity\n",
    "fig = plt.figure(figsize=(15, 8), dpi=150)\n",
    "grid = plt.GridSpec(12, 20, figure=fig, wspace=0.1, hspace=0.4)\n",
    "\n",
    "\n",
    "# Plot rastermap (keep original scaling)\n",
    "ax_raster = plt.subplot(grid[2:9, :-1])\n",
    "ax_raster.imshow(X_embedding[:, xmin_idx:xmax_idx], cmap=\"gray_r\", vmin=0, vmax=0.8, aspect=\"auto\")\n",
    "\n",
    "# Create and overlay stimulus colors\n",
    "stim_overlay = create_split_overlay(split_indices, xmin_idx, xmax_idx)\n",
    "ax_raster.imshow(stim_overlay, aspect=\"auto\")\n",
    "\n",
    "ax_raster.set_ylabel(\"Neurons (sorted)\")\n",
    "ax_raster.set_title(\"Rastermap with Stimulus Type Overlay\")\n",
    "\n",
    "ax_cbar = plt.subplot(grid[2:9, -1])\n",
    "activity_gradient = np.linspace(0.8, 0, X_embedding.shape[0])[:, np.newaxis]  # High at top\n",
    "ax_cbar.imshow(activity_gradient, cmap=\"gray_r\", aspect=\"auto\", vmin=0, vmax=0.8)\n",
    "ax_cbar.yaxis.set_label_position(\"right\")\n",
    "ax_cbar.set_ylabel(\"Activity\", rotation=270, labelpad=10)\n",
    "ax_cbar.set_yticks([0, X_embedding.shape[0]//2, X_embedding.shape[0]-1])\n",
    "ax_cbar.set_yticklabels(['0.80', '0.40', '0.00'])\n",
    "ax_cbar.set_xticks([])\n",
    "ax_cbar.yaxis.tick_right()\n",
    "\n",
    "# Plot wheel velocity below the rastermap (bigger vertical space)\n",
    "# ax_wheel = plt.subplot(grid[10:12, :-1])\n",
    "# plot_wheel_velocity(ax_wheel, preprocessed_session.aligned_wheel_velocity, timestamps, xmin_idx, xmax_idx)\n",
    "\n",
    "# Create legend for stimulus types\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor=(0.5, 0, 0.5, 0.5), label='Catch Trial'),\n",
    "    Patch(facecolor=(1, 0, 0, 0.5), label='Left 100%'),\n",
    "    Patch(facecolor=(1, 0.3, 0.3, 0.5), label='Left 25%'),\n",
    "]\n",
    "ax_raster.legend(handles=legend_elements, loc='upper right', fontsize=8, ncol=2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessed data: (19081, 6903)\n",
      "Loaded covariate matrix: (19081, 11)\n",
      "Original neurons: 7673, Processed neurons: 6903\n",
      "Data range: 0.000 to 1.000\n",
      "Covariate features: ['wheel_velocity', 'stimulus_catch_trial', 'stimulus_left_100pct', 'stimulus_left_25pct', 'stimulus_left_12.5pct', 'stimulus_left_6.25pct', 'stimulus_right_100pct', 'stimulus_right_25pct', 'stimulus_right_12.5pct', 'stimulus_right_6.25pct', 'trial_phase']\n",
      "\n",
      "Creating stimulus-based splits...\n",
      "Stimulus type 0: 62 blocks -> 43 train, 6 val, 13 test\n",
      "Stimulus type 1: 31 blocks -> 21 train, 3 val, 7 test\n",
      "Stimulus type 2: 30 blocks -> 21 train, 3 val, 6 test\n",
      "Stimulus type 3: 19 blocks -> 13 train, 1 val, 5 test\n",
      "Stimulus type 4: 35 blocks -> 24 train, 3 val, 8 test\n",
      "Stimulus type 5: 34 blocks -> 23 train, 3 val, 8 test\n",
      "Stimulus type 6: 26 blocks -> 18 train, 2 val, 6 test\n",
      "Stimulus type 7: 40 blocks -> 28 train, 4 val, 8 test\n",
      "Stimulus type 8: 36 blocks -> 25 train, 3 val, 8 test\n",
      "Generating leak-aware sample indices...\n",
      "Found 12001 valid training samples.\n",
      "Found 1942 valid validation samples.\n",
      "Found 4947 valid test samples.\n",
      "Split summary:\n",
      "  train_samples: 12001\n",
      "  val_samples: 1942\n",
      "  test_samples: 4947\n",
      "  total_samples: 18890\n",
      "  train_pct: 63.5%\n",
      "  val_pct: 10.3%\n",
      "  test_pct: 26.2%\n",
      "Selected top 5000 neurons out of 6903 preprocessed neurons\n",
      "Full dataset shape: (19081, 5000)\n",
      "Full covariate shape: (19081, 11)\n",
      "Valid train samples: 12001\n",
      "Data statistics - Mean: 0.372, Std: 0.179\n",
      "\n",
      "Dataset length: 12001\n",
      "Feature names: ['wheel_velocity', 'stimulus_catch_trial', 'stimulus_left_100pct', 'stimulus_left_25pct', 'stimulus_left_12.5pct', 'stimulus_left_6.25pct', 'stimulus_right_100pct', 'stimulus_right_25pct', 'stimulus_right_12.5pct', 'stimulus_right_6.25pct', 'trial_phase']\n",
      "Sample index: 504 -> Data index: 504\n",
      "Input sequence: 504 to 599 (96 steps)\n",
      "Target sequence start (r_begin): 552 (overlap of 48 steps)\n",
      "Target sequence: 552 to 695 (total target length: 144)\n"
     ]
    }
   ],
   "source": [
    "# Test with your session file\n",
    "dataset = Dataset_Activity(\n",
    "    root_path=\"../DATA\",\n",
    "    data_path=\"session_61f260e7-b5d3-4865-a577-bcfc53fda8a8.h5\",\n",
    "    flag='train'\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset length: {len(dataset)}\")\n",
    "print(f\"Feature names: {dataset.feature_names}\")\n",
    "\n",
    "i=504\n",
    "seq_x, seq_y, seq_x_mark, seq_y_mark = dataset[i]\n",
    "\n",
    "# print(f\"\\nSample {i}:\")\n",
    "# print(f\"  seq_x (input neural) shape: {seq_x.shape}\")\n",
    "# print(f\"  seq_y (target neural) shape: {seq_y.shape}\")  \n",
    "# print(f\"  seq_x_mark (input covariates) shape: {seq_x_mark.shape}\")\n",
    "# print(f\"  seq_y_mark (target covariates) shape: {seq_y_mark.shape}\")\n",
    "# #TODO: assert that shapes are correct.\n",
    "# print(f\"  Input wheel velocity range: {seq_x_mark[:, 0].min():.3f} to {seq_x_mark[:, 0].max():.3f}\")\n",
    "# print(f\"  Input stimulus activity: {np.sum(seq_x_mark[:, 1:10].sum(axis=1) > 0)} samples with stimulus\")\n",
    "# print(f\"  Target wheel velocity range: {seq_y_mark[:, 0].min():.3f} to {seq_y_mark[:, 0].max():.3f}\")\n",
    "# print(f\"  Target stimulus activity: {np.sum(seq_y_mark[:, 1:10].sum(axis=1) > 0)} samples with stimulus\")\n",
    "\n",
    "# print(\"\\nâœ“ Dataset_Activity successfully loads and provides covariate data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibl_env (3.10.16)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
